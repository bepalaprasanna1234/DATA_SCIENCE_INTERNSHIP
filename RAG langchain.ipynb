{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6953778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import openai\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "\n",
    "# - choose the embeddings Model\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "\n",
    "#get pdf files\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "files = [\n",
    "    \"https://www.cancer.gov/publications/patient-education/takingtime.pdf\",\n",
    "]\n",
    "\n",
    "for url in files:\n",
    "    file_path = os.path.join(\"data\", url.rpartition(\"/\")[2])\n",
    "    r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    if r.status_code == 200:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve {url}\")\n",
    "        \n",
    "#chunk it up\n",
    "loader = PyPDFDirectoryLoader(\"./data/\")\n",
    "documents = loader.load()\n",
    "# - in our testing Character split works better with this PDF data set\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    ")\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "#Look at sample embeddings\n",
    "#sample_embedding = np.array(hf.embed_query(docs[0].page_content))\n",
    "#print(\"Sample embedding of a document chunk: \", sample_embedding)\n",
    "#print(\"Size of the embedding: \", sample_embedding.shape)\n",
    "\n",
    "#create sample embeddings for the entire db\n",
    "\n",
    "vectorstore_faiss = FAISS.from_documents(\n",
    "    docs,\n",
    "    hf,\n",
    ")\n",
    "\n",
    "wrapper_store_faiss = VectorStoreIndexWrapper(vectorstore=vectorstore_faiss)\n",
    "\n",
    "query = \"What are steps to cancer survival\"\n",
    "\n",
    "#make an embedding of the query\n",
    "query_embedding = vectorstore_faiss.embedding_function(query)\n",
    "np.array(query_embedding)\n",
    "\n",
    "#now find relevant match from the corpus using the query embedding\n",
    "relevant_documents = vectorstore_faiss.similarity_search_by_vector(query_embedding)\n",
    "print(f'{len(relevant_documents)} documents are fetched which are relevant to the query.')\n",
    "print('----')\n",
    "answer = []\n",
    "for i, rel_doc in enumerate(relevant_documents):\n",
    "    print(f'## Document {i+1}: {rel_doc.page_content}.......')\n",
    "    answer.append(rel_doc.page_content)\n",
    "\n",
    "#concatenate the answers             \n",
    "answer = ''.join(answer)\n",
    "    \n",
    "# Set up your open API key and endpoint\n",
    "openai.api_key = \"here goes you api\" #enter your API here\n",
    "\n",
    "if answer:  # This checks if the answers list is not empty\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"davinci\",\n",
    "        prompt=f\"The document says: '{answer}'. What is the main idea?\",\n",
    "        max_tokens=200,\n",
    "        temperature=1,\n",
    "    )\n",
    "    print(\"ChatGPT Interpretation:\")\n",
    "    print(response.choices[0].text.strip())\n",
    "else:\n",
    "    print(\"No results noted in local query database.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
